# train
train.preset=train.online
#train.preset=train.batchrun
#train.preset=train.batchrun2


# agent


#agent.preset=agent_cleanrl2  #立ち上がり早いが安定しない
#agent.preset=agent_cleanrl4  #結果だけみるといい感じ
#agent.preset=agent_nrb
#agent.preset=agent_nrb		# ReplayBuffer無し

agent.preset=agent_asdqn1

#-------Adaptive Stabilized DQN (AS-DQN)

agent_asdqn1.alpha=1e-3            # ★ 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_asdqn1.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_asdqn1.eps_max=1.00          # 指数減衰  
agent_asdqn1.eps_min=0.05          # 0.1f 0.05f
agent_asdqn1.eps_decay_step=100000 # 100K
agent_asdqn1.softupdate_tau=0.01   # 1.0f 0.004f  0.01f 0.005f;
agent_asdqn1.hardupdate_step=-1    #  -1 5000; //200 500 1000
agent_asdqn1.use_grad_clip=false
agent_asdqn1.grad_clip_tau=-1    # -1 10~40 1f 5f 10f
agent_asdqn1.use_td_clip=false
agent_asdqn1.td_clip_value=4.0
agent_asdqn1.eps_zero_step=-1      #120000;
agent_asdqn1.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_asdqn1.use_replay_buffer=true #
agent_asdqn1.replay_capacity=10000
agent_asdqn1.replay_batch_size=128
agent_asdqn1.replay_warmup_steps=10000 #
agent_asdqn1.replay_update_interval=10 #
agent_asdqn1.use_as_dqn = true  //        # ★Adaptive Stabilized DQN (AS-DQN)
agent_asdqn1.qstd_alpha = 0.01           #Q値 std の EWMA 平滑率
agent_asdqn1.q_z_threshold = 3.0         #z-score 崩壊判定閾値
agent_asdqn1.q_cv_threshold = 0.5        //CV 崩壊判定閾値
agent_asdqn1.q_niqr_threshold = 0.6      //NIQR 崩壊判定閾値
agent_asdqn1.eps_boost_max = 2.0         #ε ブースト上限倍率
agent_asdqn1.eps_boost_half_life = 10000  # ε ブーストの自然減衰半減期
agent_asdqn1.tau_min = 0.0005            #τ の下限
agent_asdqn1.tau_max = 0.01            #τ の上限
agent_asdqn1.tau_decay_on_hit = 0.98    # 1stepごとに+2%減衰
agent_asdqn1.tau_recover_rate = 0.002   # 1stepごとに+0.2%回復
agent_asdqn1.tau_recover_delay = 1000     # 1000step安定していたら回復開始

#-------

## リプレイバッファ有 比較検証

agent_cleanrl2.alpha=1e-3            # ★ 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_cleanrl2.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_cleanrl2.eps_max=1.00  # 指数減衰  
agent_cleanrl2.eps_min=0.05          # 0.1f 0.05f
agent_cleanrl2.eps_decay_step=250000 #
agent_cleanrl2.softupdate_tau=-1  # 1.0f 0.004f  0.01f 0.005f;
agent_cleanrl2.hardupdate_step=500  #  -1 5000; //200 500 1000
agent_cleanrl2.use_grad_clip=false
agent_cleanrl2.grad_clip_tau=-1    # -1 10~40 1f 5f 10f
agent_cleanrl2.use_td_clip=false
agent_cleanrl2.td_clip_value=4.0
agent_cleanrl2.eps_zero_step=-1      #120000;
agent_cleanrl2.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_cleanrl2.use_replay_buffer=true #
agent_cleanrl2.replay_capacity=10000
agent_cleanrl2.replay_batch_size=128
agent_cleanrl2.replay_warmup_steps=10000 #
agent_cleanrl2.replay_update_interval=10 #

### ①CleanRL オリジナル 結果の立ち上がりが遅い
agent_cleanrl.alpha=2.5e-4            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_cleanrl.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_cleanrl.eps_max=1.00  # 指数減衰  
agent_cleanrl.eps_min=0.05          # 0.1f 0.05f
agent_cleanrl.eps_decay_step=250000 #
agent_cleanrl.softupdate_tau=-1  # 1.0f 0.004f  0.01f 0.005f;
agent_cleanrl.hardupdate_step=500  #  -1 5000; //200 500 1000
agent_cleanrl.use_grad_clip=false
agent_cleanrl.grad_clip_tau=-1    # -1 10~40 1f 5f 10f
agent_cleanrl.use_td_clip=false
agent_cleanrl.td_clip_value=4.0
agent_cleanrl.eps_zero_step=-1      #120000;
agent_cleanrl.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_cleanrl.use_replay_buffer=true #
agent_cleanrl.replay_capacity=10000
agent_cleanrl.replay_batch_size=128
agent_cleanrl.replay_warmup_steps=10000 #
agent_cleanrl.replay_update_interval=10 #

### ② 立ち上がり早いが安定しない
agent_cleanrl2.alpha=1e-3            # ★ 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_cleanrl2.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_cleanrl2.eps_max=1.00  # 指数減衰  
agent_cleanrl2.eps_min=0.05          # 0.1f 0.05f
agent_cleanrl2.eps_decay_step=250000 #
agent_cleanrl2.softupdate_tau=-1  # 1.0f 0.004f  0.01f 0.005f;
agent_cleanrl2.hardupdate_step=500  #  -1 5000; //200 500 1000
agent_cleanrl2.use_grad_clip=false
agent_cleanrl2.grad_clip_tau=-1    # -1 10~40 1f 5f 10f
agent_cleanrl2.use_td_clip=false
agent_cleanrl2.td_clip_value=4.0
agent_cleanrl2.eps_zero_step=-1      #120000;
agent_cleanrl2.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_cleanrl2.use_replay_buffer=true #
agent_cleanrl2.replay_capacity=10000
agent_cleanrl2.replay_batch_size=128
agent_cleanrl2.replay_warmup_steps=10000 #
agent_cleanrl2.replay_update_interval=10 #

### ③
agent_cleanrl3.alpha=1e-4            # ★ 学習率 1e-3 3e-3 5e-4 3e-4 1e-4
agent_cleanrl3.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_cleanrl3.eps_max=1.00  # 指数減衰  
agent_cleanrl3.eps_min=0.05          # 0.1f 0.05f
agent_cleanrl3.eps_decay_step=250000 #
agent_cleanrl3.softupdate_tau=-1  # 1.0f 0.004f  0.01f 0.005f;
agent_cleanrl3.hardupdate_step=500  #  -1 5000; //200 500 1000
agent_cleanrl3.use_grad_clip=false
agent_cleanrl3.grad_clip_tau=-1    # -1 10~40 1f 5f 10f
agent_cleanrl3.use_td_clip=false
agent_cleanrl3.td_clip_value=4.0
agent_cleanrl3.eps_zero_step=-1      #120000;
agent_cleanrl3.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_cleanrl3.use_replay_buffer=true #
agent_cleanrl3.replay_capacity=10000
agent_cleanrl3.replay_batch_size=128
agent_cleanrl3.replay_warmup_steps=10000 #
agent_cleanrl3.replay_update_interval=10 #

### ④ 結果はいい感じ
agent_cleanrl4.alpha=2.5e-4          # 学習率 1e-3 3e-3 5e-4 3e-4 1e-4
agent_cleanrl4.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_cleanrl4.eps_max=1.00  # 指数減衰  
agent_cleanrl4.eps_min=0.05          # 0.1f 0.05f
agent_cleanrl4.eps_decay_step=100000 # ★
agent_cleanrl4.softupdate_tau=-1  # 1.0f 0.004f  0.01f 0.005f;
agent_cleanrl4.hardupdate_step=500  #  -1 5000; //200 500 1000
agent_cleanrl4.use_grad_clip=false
agent_cleanrl4.grad_clip_tau=-1    # -1 10~40 1f 5f 10f
agent_cleanrl4.use_td_clip=false
agent_cleanrl4.td_clip_value=4.0
agent_cleanrl4.eps_zero_step=-1      #120000;
agent_cleanrl4.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_cleanrl4.use_replay_buffer=true #
agent_cleanrl4.replay_capacity=10000
agent_cleanrl4.replay_batch_size=128
agent_cleanrl4.replay_warmup_steps=10000 #
agent_cleanrl4.replay_update_interval=10 #

### ⑤
agent_cleanrl5.alpha=2.5e-4          # 学習率 1e-3 3e-3 5e-4 3e-4 1e-4
agent_cleanrl5.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_cleanrl5.eps_max=1.00  # 指数減衰  
agent_cleanrl5.eps_min=0.05          # 0.1f 0.05f
agent_cleanrl5.eps_decay_step=50000 # ★
agent_cleanrl5.softupdate_tau=-1  # 1.0f 0.004f  0.01f 0.005f;
agent_cleanrl5.hardupdate_step=500  #  -1 5000; //200 500 1000
agent_cleanrl5.use_grad_clip=false
agent_cleanrl5.grad_clip_tau=-1    # -1 10~40 1f 5f 10f
agent_cleanrl5.use_td_clip=false
agent_cleanrl5.td_clip_value=4.0
agent_cleanrl5.eps_zero_step=-1      #120000;
agent_cleanrl5.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_cleanrl5.use_replay_buffer=true #
agent_cleanrl5.replay_capacity=10000
agent_cleanrl5.replay_batch_size=128
agent_cleanrl5.replay_warmup_steps=10000 #
agent_cleanrl5.replay_update_interval=10 #

### ⑥
agent_cleanrl6.alpha=2.5e-4          # 学習率 1e-3 3e-3 5e-4 3e-4 1e-4
agent_cleanrl6.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_cleanrl6.eps_max=1.00  # 指数減衰  
agent_cleanrl6.eps_min=0.05          # 0.1f 0.05f
agent_cleanrl6.eps_decay_step=250000 # 
agent_cleanrl6.softupdate_tau=-1  # 1.0f 0.004f  0.01f 0.005f;
agent_cleanrl6.hardupdate_step=500  #  -1 5000; //200 500 1000
agent_cleanrl6.use_grad_clip=true  # ★
agent_cleanrl6.grad_clip_tau=30    # -1 30 10~40 1f 5f 10f
agent_cleanrl6.use_td_clip=false
agent_cleanrl6.td_clip_value=4.0
agent_cleanrl6.eps_zero_step=-1      #120000;
agent_cleanrl6.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_cleanrl6.use_replay_buffer=true #
agent_cleanrl6.replay_capacity=10000
agent_cleanrl6.replay_batch_size=128
agent_cleanrl6.replay_warmup_steps=10000 #
agent_cleanrl6.replay_update_interval=10 #


### 修正後 ①メイン1回目
agent_rb1.alpha=1e-3            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_rb1.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_rb1.eps_max=1.00
agent_rb1.eps_min=0.02          # ★0.1f 0.05f
agent_rb1.eps_decay_step=20000  # ★ 
agent_rb1.softupdate_tau=0.015  # 1.0f 0.004f  0.01f 0.005f;
agent_rb1.hardupdate_step=-1    # ★ -1 5000; //200 500 1000
agent_rb1.use_grad_clip=true
agent_rb1.grad_clip_tau=30.0    # 10~40 1f 5f 10f
agent_rb1.use_td_clip=true
agent_rb1.td_clip_value=4.0
agent_rb1.eps_zero_step=-1      #120000;
agent_rb1.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_rb1.use_replay_buffer=true  #
agent_rb1.replay_capacity=50000
agent_rb1.replay_batch_size=64
agent_rb1.replay_warmup_steps=500  #★
agent_rb1.replay_update_interval=1 #★

### 修正後 ② ①からアルファ下げ
agent_rb2.alpha=3e-4            #★ 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_rb2.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_rb2.eps_max=1.00
agent_rb2.eps_min=0.02          #0.1f 0.05f
agent_rb2.eps_decay_step=20000
agent_rb2.softupdate_tau=0.015  # 1.0f 0.004f  0.01f 0.005f;
agent_rb2.hardupdate_step=-1  # -1 5000; //200 500 1000
agent_rb2.use_grad_clip=true
agent_rb2.grad_clip_tau=30.0    # 10~40 1f 5f 10f
agent_rb2.use_td_clip=true
agent_rb2.td_clip_value=4.0
agent_rb2.eps_zero_step=-1      #120000;
agent_rb2.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_rb2.use_replay_buffer=true  #
agent_rb2.replay_capacity=50000
agent_rb2.replay_batch_size=64
agent_rb2.replay_warmup_steps=500
agent_rb2.replay_update_interval=1

### 修正前③
agent_rb3.alpha=1e-3            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_rb3.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_rb3.eps_max=1.00
agent_rb3.eps_min=0.05          #0.1f 0.05f
agent_rb3.eps_decay_step=100000
agent_rb3.softupdate_tau=0.015  # 1.0f 0.004f  0.01f 0.005f;
agent_rb3.hardupdate_step=2000  # -1 5000; //200 500 1000
agent_rb3.use_grad_clip=true
agent_rb3.grad_clip_tau=30.0    # ★ 10~40 1f 5f 10f
agent_rb3.use_td_clip=true
agent_rb3.td_clip_value=4.0
agent_rb3.eps_zero_step=-1      #120000;
agent_rb3.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_rb3.use_replay_buffer=true #
agent_rb3.replay_capacity=50000
agent_rb3.replay_batch_size=64
agent_rb3.replay_warmup_steps=1000
agent_rb3.replay_update_interval=4

### 修正後 ⑤hardupdate_stepだけ適用
agent_rb5.alpha=1e-3            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_rb5.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_rb5.eps_max=1.00
agent_rb5.eps_min=0.05          #0.1f 0.05f
agent_rb5.eps_decay_step=100000
agent_rb5.softupdate_tau=0.015  # 1.0f 0.004f  0.01f 0.005f;
agent_rb5.hardupdate_step=-1    # ★ -1 5000; //200 500 1000
agent_rb5.use_grad_clip=true
agent_rb5.grad_clip_tau=30.0    # 10~40 1f 5f 10f
agent_rb5.use_td_clip=true
agent_rb5.td_clip_value=4.0
agent_rb5.eps_zero_step=-1      #120000;
agent_rb5.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_rb5.use_replay_buffer=true #
agent_rb5.replay_capacity=50000
agent_rb5.replay_batch_size=64
agent_rb5.replay_warmup_steps=1000
agent_rb5.replay_update_interval=4

### 修正後 ⑥epsだけ適用
agent_rb6.alpha=1e-3            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_rb6.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_rb6.eps_max=1.00         
agent_rb6.eps_min=0.02          #★ 0.1f 0.05f
agent_rb6.eps_decay_step=20000  #★
agent_rb6.softupdate_tau=0.015  # 1.0f 0.004f  0.01f 0.005f;
agent_rb6.hardupdate_step=2000  #  -1 5000; //200 500 1000
agent_rb6.use_grad_clip=true
agent_rb6.grad_clip_tau=30.0    # 10~40 1f 5f 10f
agent_rb6.use_td_clip=true
agent_rb6.td_clip_value=4.0
agent_rb6.eps_zero_step=-1      #120000;
agent_rb6.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_rb6.use_replay_buffer=true #
agent_rb6.replay_capacity=50000
agent_rb6.replay_batch_size=64
agent_rb6.replay_warmup_steps=1000
agent_rb6.replay_update_interval=4

### 修正後 ⑦hardupdate_stepだけもとに戻す
agent_rb7.alpha=1e-3            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_rb7.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_rb7.eps_max=1.00
agent_rb7.eps_min=0.02          #0.1f 0.05f
agent_rb7.eps_decay_step=20000
agent_rb7.softupdate_tau=0.015  # 1.0f 0.004f  0.01f 0.005f;
agent_rb7.hardupdate_step=500  #★ -1 5000; //200 500 1000
agent_rb7.grad_clip_tau=30.0    # 10~40 1f 5f 10f
agent_rb7.use_td_clip=true
agent_rb7.td_clip_value=4.0
agent_rb7.eps_zero_step=-1      #120000;
agent_rb7.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_rb7.use_replay_buffer=true  #
agent_rb7.replay_capacity=50000
agent_rb7.replay_batch_size=64
agent_rb7.replay_warmup_steps=500
agent_rb7.replay_update_interval=1

## リプレイバッファ無し
agent_nrb.alpha=3e-4            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_nrb.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_nrb.eps_max=1.00
agent_nrb.eps_min=0.05          #0.1f 0.05f
agent_nrb.eps_decay_step=100000
agent_nrb.softupdate_tau=0.005  # 1.0f 0.004f  0.01f 0.005f;
agent_nrb.hardupdate_step=5000  # -1 5000; //200 500 1000
agent_nrb.use_grad_clip=true
agent_nrb.grad_clip_tau=30.0    # 10~40 1f 5f 10f
agent_nrb.use_td_clip=true
agent_nrb.td_clip_value=2.0
agent_nrb.eps_zero_step=-1      #120000;
agent_nrb.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_nrb.use_replay_buffer=false   #★

agent_nrb2.alpha=1e-3            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_nrb2.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_nrb2.eps_max=1.00
agent_nrb2.eps_min=0.05          #0.1f 0.05f
agent_nrb2.eps_decay_step=100000
agent_nrb2.softupdate_tau=0.015  # 1.0f 0.004f  0.01f 0.005f;
agent_nrb2.hardupdate_step=2000  # -1 5000; //200 500 1000
agent_nrb2.use_grad_clip=true
agent_nrb2.grad_clip_tau=30.0    # 10~40 1f 5f 10f
agent_nrb2.use_td_clip=true
agent_nrb2.td_clip_value=4.0
agent_nrb2.eps_zero_step=-1      #120000;
agent_nrb2.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_nrb2.use_replay_buffer=false   #★



## デフォルト設定
agent.alpha=1e-3            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent.eps_max=1.00
agent.eps_min=0.05          #0.1f 0.05f
agent.eps_decay_step=100000
agent.softupdate_tau=0.015  # 1.0f 0.004f  0.01f 0.005f;   // 大きいとターゲットネットワークからの反映が早くなる。小さいと遅く滑らかになる。0.005→半減期138step
agent.hardupdate_step=2000  # -1 5000; //200 500 1000
agent.grad_clip_tau=30.0    # 10~40 1f 5f 10f
agent.use_td_clip=true
agent.td_clip_value=4.0
agent.eps_zero_step=-1      #120000;
agent.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent.use_replay_buffer=true
agent.replay_capacity=50000
agent.replay_batch_size=64
agent.replay_warmup_steps=1000
agent.replay_update_interval=4
agent.heatmap_log_image_interval = 100
agent.heatmap_log_sweep_interval = 100
agent.heatmap_log_hist_interval = 10
agent.use_as_dqn = false             //Adaptive Stabilized DQN (AS-DQN)
agent.qstd_alpha = 0.01           //Q値 std の EWMA 平滑率
agent.q_z_threshold = 3.0         //z-score 崩壊判定閾値
agent.q_cv_threshold = 0.5        //CV 崩壊判定閾値
agent.q_niqr_threshold = 0.6      //NIQR 崩壊判定閾値
agent.eps_boost_max = 2.0         //ε ブースト上限倍率
agent.eps_boost_half_life = 10000  //ε ブーストの自然減衰半減期
agent.tau_min = 0.0005            //τ の下限
agent_asdqn1.tau_max = 0.01            #τ の上限
agent.tau_decay_on_hit = 0.98   # 1stepごとに+0.2%回復
agent.tau_recover_rate = 0.002   // 1stepごとに+0.2%回復
agent.tau_recover_delay = 1000     // 1000step安定していたら回復開始



## train default
train.timer_ms = 50
train.step_per_frame = 10
train.eval_interval = 1
train.train_pause_step = 110000 # -1
#train.train_exit_step = -1 # -1 110000

## ほぼほぼエピソード単位で描画更新
train.online.timer_ms = 10
train.online.step_per_frame = 100
train.online.eval_interval = 1
train.online.train_pause_step = 500000 #500K

## バッチ実行用①
train.batchrun.timer_ms = 10
train.batchrun.step_per_frame = 1000
train.batchrun.train_pause_step = -1
train.batchrun.train_exit_step = 500000

## バッチ実行用②
train.batchrun2.timer_ms = 10
train.batchrun2.step_per_frame = 1000
train.batchrun2.train_pause_step = -1
train.batchrun2.train_exit_step = 500000  # -1 110000
