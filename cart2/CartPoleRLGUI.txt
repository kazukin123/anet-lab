# agent

agent.preset=agent_nrb
#agent.preset=agent_nrb		# ReplayBuffer無し


## リプレイバッファ有 比較検証

### ①CleanRL オリジナル
agent_cleanrl.alpha=2.5e-4            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_cleanrl.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_cleanrl.eps_max=1.00  # 指数減衰  
agent_cleanrl.eps_min=0.05          # 0.1f 0.05f
agent_cleanrl.eps_decay_step=250000 #
agent_cleanrl.softupdate_tau=-1  # 1.0f 0.004f  0.01f 0.005f;
agent_cleanrl.hardupdate_step=500  #  -1 5000; //200 500 1000
agent_cleanrl.use_grad_clip=false
agent_cleanrl.grad_clip_tau=-1    # -1 10~40 1f 5f 10f
agent_cleanrl.use_td_clip=false
agent_cleanrl.td_clip_value=4.0
agent_cleanrl.eps_zero_step=-1      #120000;
agent_cleanrl.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_cleanrl.use_replay_buffer=true #
agent_cleanrl.replay_capacity=10000
agent_cleanrl.replay_batch_size=128
agent_cleanrl.replay_warmup_steps=10000 #
agent_cleanrl.replay_update_interval=10 #

### ②
agent_cleanrl2.alpha=1e-3            # ★ 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_cleanrl2.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_cleanrl2.eps_max=1.00  # 指数減衰  
agent_cleanrl2.eps_min=0.05          # 0.1f 0.05f
agent_cleanrl2.eps_decay_step=250000 #
agent_cleanrl2.softupdate_tau=-1  # 1.0f 0.004f  0.01f 0.005f;
agent_cleanrl2.hardupdate_step=500  #  -1 5000; //200 500 1000
agent_cleanrl2.use_grad_clip=false
agent_cleanrl2.grad_clip_tau=-1    # -1 10~40 1f 5f 10f
agent_cleanrl2.use_td_clip=false
agent_cleanrl2.td_clip_value=4.0
agent_cleanrl2.eps_zero_step=-1      #120000;
agent_cleanrl2.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_cleanrl2.use_replay_buffer=true #
agent_cleanrl2.replay_capacity=10000
agent_cleanrl2.replay_batch_size=128
agent_cleanrl2.replay_warmup_steps=10000 #
agent_cleanrl2.replay_update_interval=10 #

### ③
agent_cleanrl3.alpha=1e-4            # ★ 学習率 1e-3 3e-3 5e-4 3e-4 1e-4
agent_cleanrl3.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_cleanrl3.eps_max=1.00  # 指数減衰  
agent_cleanrl3.eps_min=0.05          # 0.1f 0.05f
agent_cleanrl3.eps_decay_step=250000 #
agent_cleanrl3.softupdate_tau=-1  # 1.0f 0.004f  0.01f 0.005f;
agent_cleanrl3.hardupdate_step=500  #  -1 5000; //200 500 1000
agent_cleanrl3.use_grad_clip=false
agent_cleanrl3.grad_clip_tau=-1    # -1 10~40 1f 5f 10f
agent_cleanrl3.use_td_clip=false
agent_cleanrl3.td_clip_value=4.0
agent_cleanrl3.eps_zero_step=-1      #120000;
agent_cleanrl3.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_cleanrl3.use_replay_buffer=true #
agent_cleanrl3.replay_capacity=10000
agent_cleanrl3.replay_batch_size=128
agent_cleanrl3.replay_warmup_steps=10000 #
agent_cleanrl3.replay_update_interval=10 #

### ④
agent_cleanrl4.alpha=2.5e-4          # 学習率 1e-3 3e-3 5e-4 3e-4 1e-4
agent_cleanrl4.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_cleanrl4.eps_max=1.00  # 指数減衰  
agent_cleanrl4.eps_min=0.05          # 0.1f 0.05f
agent_cleanrl4.eps_decay_step=100000 # ★
agent_cleanrl4.softupdate_tau=-1  # 1.0f 0.004f  0.01f 0.005f;
agent_cleanrl4.hardupdate_step=500  #  -1 5000; //200 500 1000
agent_cleanrl4.use_grad_clip=false
agent_cleanrl4.grad_clip_tau=-1    # -1 10~40 1f 5f 10f
agent_cleanrl4.use_td_clip=false
agent_cleanrl4.td_clip_value=4.0
agent_cleanrl4.eps_zero_step=-1      #120000;
agent_cleanrl4.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_cleanrl4.use_replay_buffer=true #
agent_cleanrl4.replay_capacity=10000
agent_cleanrl4.replay_batch_size=128
agent_cleanrl4.replay_warmup_steps=10000 #
agent_cleanrl4.replay_update_interval=10 #

### ⑤
agent_cleanrl5.alpha=2.5e-4          # 学習率 1e-3 3e-3 5e-4 3e-4 1e-4
agent_cleanrl5.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_cleanrl5.eps_max=1.00  # 指数減衰  
agent_cleanrl5.eps_min=0.05          # 0.1f 0.05f
agent_cleanrl5.eps_decay_step=50000 # ★
agent_cleanrl5.softupdate_tau=-1  # 1.0f 0.004f  0.01f 0.005f;
agent_cleanrl5.hardupdate_step=500  #  -1 5000; //200 500 1000
agent_cleanrl5.use_grad_clip=false
agent_cleanrl5.grad_clip_tau=-1    # -1 10~40 1f 5f 10f
agent_cleanrl5.use_td_clip=false
agent_cleanrl5.td_clip_value=4.0
agent_cleanrl5.eps_zero_step=-1      #120000;
agent_cleanrl5.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_cleanrl5.use_replay_buffer=true #
agent_cleanrl5.replay_capacity=10000
agent_cleanrl5.replay_batch_size=128
agent_cleanrl5.replay_warmup_steps=10000 #
agent_cleanrl5.replay_update_interval=10 #

### ⑥
agent_cleanrl6.alpha=2.5e-4          # 学習率 1e-3 3e-3 5e-4 3e-4 1e-4
agent_cleanrl6.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_cleanrl6.eps_max=1.00  # 指数減衰  
agent_cleanrl6.eps_min=0.05          # 0.1f 0.05f
agent_cleanrl6.eps_decay_step=250000 # 
agent_cleanrl6.softupdate_tau=-1  # 1.0f 0.004f  0.01f 0.005f;
agent_cleanrl6.hardupdate_step=500  #  -1 5000; //200 500 1000
agent_cleanrl6.use_grad_clip=true  # ★
agent_cleanrl6.grad_clip_tau=30    # -1 30 10~40 1f 5f 10f
agent_cleanrl6.use_td_clip=false
agent_cleanrl6.td_clip_value=4.0
agent_cleanrl6.eps_zero_step=-1      #120000;
agent_cleanrl6.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_cleanrl6.use_replay_buffer=true #
agent_cleanrl6.replay_capacity=10000
agent_cleanrl6.replay_batch_size=128
agent_cleanrl6.replay_warmup_steps=10000 #
agent_cleanrl6.replay_update_interval=10 #


### 修正後 ①メイン1回目
agent_rb1.alpha=1e-3            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_rb1.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_rb1.eps_max=1.00
agent_rb1.eps_min=0.02          # ★0.1f 0.05f
agent_rb1.eps_decay_step=20000  # ★ 
agent_rb1.softupdate_tau=0.015  # 1.0f 0.004f  0.01f 0.005f;
agent_rb1.hardupdate_step=-1    # ★ -1 5000; //200 500 1000
agent_rb1.use_grad_clip=true
agent_rb1.grad_clip_tau=30.0    # 10~40 1f 5f 10f
agent_rb1.use_td_clip=true
agent_rb1.td_clip_value=4.0
agent_rb1.eps_zero_step=-1      #120000;
agent_rb1.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_rb1.use_replay_buffer=true  #
agent_rb1.replay_capacity=50000
agent_rb1.replay_batch_size=64
agent_rb1.replay_warmup_steps=500  #★
agent_rb1.replay_update_interval=1 #★

### 修正後 ② ①からアルファ下げ
agent_rb2.alpha=3e-4            #★ 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_rb2.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_rb2.eps_max=1.00
agent_rb2.eps_min=0.02          #0.1f 0.05f
agent_rb2.eps_decay_step=20000
agent_rb2.softupdate_tau=0.015  # 1.0f 0.004f  0.01f 0.005f;
agent_rb2.hardupdate_step=-1  # -1 5000; //200 500 1000
agent_rb2.use_grad_clip=true
agent_rb2.grad_clip_tau=30.0    # 10~40 1f 5f 10f
agent_rb2.use_td_clip=true
agent_rb2.td_clip_value=4.0
agent_rb2.eps_zero_step=-1      #120000;
agent_rb2.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_rb2.use_replay_buffer=true  #
agent_rb2.replay_capacity=50000
agent_rb2.replay_batch_size=64
agent_rb2.replay_warmup_steps=500
agent_rb2.replay_update_interval=1

### 修正前③
agent_rb3.alpha=1e-3            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_rb3.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_rb3.eps_max=1.00
agent_rb3.eps_min=0.05          #0.1f 0.05f
agent_rb3.eps_decay_step=100000
agent_rb3.softupdate_tau=0.015  # 1.0f 0.004f  0.01f 0.005f;
agent_rb3.hardupdate_step=2000  # -1 5000; //200 500 1000
agent_rb3.use_grad_clip=true
agent_rb3.grad_clip_tau=30.0    # ★ 10~40 1f 5f 10f
agent_rb3.use_td_clip=true
agent_rb3.td_clip_value=4.0
agent_rb3.eps_zero_step=-1      #120000;
agent_rb3.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_rb3.use_replay_buffer=true #
agent_rb3.replay_capacity=50000
agent_rb3.replay_batch_size=64
agent_rb3.replay_warmup_steps=1000
agent_rb3.replay_update_interval=4

### 修正後 ⑤hardupdate_stepだけ適用
agent_rb5.alpha=1e-3            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_rb5.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_rb5.eps_max=1.00
agent_rb5.eps_min=0.05          #0.1f 0.05f
agent_rb5.eps_decay_step=100000
agent_rb5.softupdate_tau=0.015  # 1.0f 0.004f  0.01f 0.005f;
agent_rb5.hardupdate_step=-1    # ★ -1 5000; //200 500 1000
agent_rb5.use_grad_clip=true
agent_rb5.grad_clip_tau=30.0    # 10~40 1f 5f 10f
agent_rb5.use_td_clip=true
agent_rb5.td_clip_value=4.0
agent_rb5.eps_zero_step=-1      #120000;
agent_rb5.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_rb5.use_replay_buffer=true #
agent_rb5.replay_capacity=50000
agent_rb5.replay_batch_size=64
agent_rb5.replay_warmup_steps=1000
agent_rb5.replay_update_interval=4

### 修正後 ⑥epsだけ適用
agent_rb6.alpha=1e-3            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_rb6.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_rb6.eps_max=1.00         
agent_rb6.eps_min=0.02          #★ 0.1f 0.05f
agent_rb6.eps_decay_step=20000  #★
agent_rb6.softupdate_tau=0.015  # 1.0f 0.004f  0.01f 0.005f;
agent_rb6.hardupdate_step=2000  #  -1 5000; //200 500 1000
agent_rb6.use_grad_clip=true
agent_rb6.grad_clip_tau=30.0    # 10~40 1f 5f 10f
agent_rb6.use_td_clip=true
agent_rb6.td_clip_value=4.0
agent_rb6.eps_zero_step=-1      #120000;
agent_rb6.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_rb6.use_replay_buffer=true #
agent_rb6.replay_capacity=50000
agent_rb6.replay_batch_size=64
agent_rb6.replay_warmup_steps=1000
agent_rb6.replay_update_interval=4

### 修正後 ⑦hardupdate_stepだけもとに戻す
agent_rb7.alpha=1e-3            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_rb7.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_rb7.eps_max=1.00
agent_rb7.eps_min=0.02          #0.1f 0.05f
agent_rb7.eps_decay_step=20000
agent_rb7.softupdate_tau=0.015  # 1.0f 0.004f  0.01f 0.005f;
agent_rb7.hardupdate_step=500  #★ -1 5000; //200 500 1000
agent_rb7.grad_clip_tau=30.0    # 10~40 1f 5f 10f
agent_rb7.use_td_clip=true
agent_rb7.td_clip_value=4.0
agent_rb7.eps_zero_step=-1      #120000;
agent_rb7.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_rb7.use_replay_buffer=true  #
agent_rb7.replay_capacity=50000
agent_rb7.replay_batch_size=64
agent_rb7.replay_warmup_steps=500
agent_rb7.replay_update_interval=1

## リプレイバッファ無し
agent_nrb.alpha=3e-4            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_nrb.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_nrb.eps_max=1.00
agent_nrb.eps_min=0.05          #0.1f 0.05f
agent_nrb.eps_decay_step=100000
agent_nrb.softupdate_tau=0.005  # 1.0f 0.004f  0.01f 0.005f;
agent_nrb.hardupdate_step=5000  # -1 5000; //200 500 1000
agent_nrb.use_grad_clip=true
agent_nrb.grad_clip_tau=30.0    # 10~40 1f 5f 10f
agent_nrb.use_td_clip=true
agent_nrb.td_clip_value=2.0
agent_nrb.eps_zero_step=-1      #120000;
agent_nrb.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_nrb.use_replay_buffer=false   #★

agent_nrb2.alpha=1e-3            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent_nrb2.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent_nrb2.eps_max=1.00
agent_nrb2.eps_min=0.05          #0.1f 0.05f
agent_nrb2.eps_decay_step=100000
agent_nrb2.softupdate_tau=0.015  # 1.0f 0.004f  0.01f 0.005f;
agent_nrb2.hardupdate_step=2000  # -1 5000; //200 500 1000
agent_nrb2.use_grad_clip=true
agent_nrb2.grad_clip_tau=30.0    # 10~40 1f 5f 10f
agent_nrb2.use_td_clip=true
agent_nrb2.td_clip_value=4.0
agent_nrb2.eps_zero_step=-1      #120000;
agent_nrb2.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent_nrb2.use_replay_buffer=false   #★



## デフォルト設定
agent.alpha=1e-3            # 学習率 1e-3 3e-3 1e-4 1e-4 3e-4 5e-4
agent.gamma=0.99            # 0.99f; 0.995f      γが高いほど「長期安定」を目指す
agent.eps_max=1.00
agent.eps_min=0.05          #0.1f 0.05f
agent.eps_decay_step=100000
agent.softupdate_tau=0.015  # 1.0f 0.004f  0.01f 0.005f;   // 大きいとターゲットネットワークからの反映が早くなる。小さいと遅く滑らかになる。0.005→半減期138step
agent.hardupdate_step=2000  # -1 5000; //200 500 1000
agent.grad_clip_tau=30.0    # 10~40 1f 5f 10f
agent.use_td_clip=true
agent.td_clip_value=4.0
agent.eps_zero_step=-1      #120000;
agent.use_double_dqn=true   # Double DQN 有効化フラグ（trueで有効）
agent.use_replay_buffer=true
agent.replay_capacity=50000
agent.replay_batch_size=64
agent.replay_warmup_steps=1000
agent.replay_update_interval=4
agent.heatmap_log_image_interval = 100
agent.heatmap_log_sweep_interval = 100



# train
#train.preset=train.eptime
#train.preset=train.batchrun
#train.preset=train.batchrun2


## train default
train.timer_ms = 50
train.step_per_frame = 10
train.eval_interval = 1
train.train_pause_step = 110000 # -1
#train.train_exit_step = -1 # -1 110000

## ほぼほぼエピソード単位で描画更新
train.eptime.timer_ms = 10
train.eptime.step_per_frame = 1000
train.eptime.eval_interval = 1
train.eptime.train_pause_step = 110000 # -1

## バッチ実行用①
train.batchrun.timer_ms = 1
train.batchrun.step_per_frame = 10000
train.batchrun.train_pause_step = -1
train.batchrun.train_exit_step = 110000

## バッチ実行用②
train.batchrun2.timer_ms = 10
train.batchrun2.step_per_frame = 1000
train.batchrun2.train_pause_step = -1
train.batchrun2.train_exit_step = 500000  # -1 110000
